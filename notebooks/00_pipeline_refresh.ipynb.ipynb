{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ef8c9a",
   "metadata": {},
   "source": [
    "# Data Pipeline Refresh Playbook\n",
    "\n",
    "**Purpose:** Re-run source ingestion and keep derived datasets/docs in sync when upstream CSVs change.\n",
    "\n",
    "**Checklist:**\n",
    "1) Pull latest raw sources into `data/original` (SATCAT, UCS, countries.geojson).\n",
    "2) Quick diff: row counts + schema drift; snapshot versions/date ranges.\n",
    "3) Re-run cleaning: `01_ucs_cleanup` ‚Üí `02_satcat_cleanup` ‚Üí `03_orbital_risk_synthesis`.\n",
    "4) Regenerate outputs: `ucs_cleaned.csv`, `satcat_cleaned.csv`, `kinetic_master.csv`.\n",
    "5) Refresh visuals: rerun plotting cells to update `/images` exports.\n",
    "6) Update docs: README stats (objects, mass, KE, zombies, velocity) + figures captions if changed.\n",
    "7) Log run metadata (source dates, hashes) in this notebook for traceability.\n",
    "\n",
    "**Next step:** wire a small automation cell here to run the above sequence end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9788dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Saving data to: d:\\repos\\orbital-debris-assessment\\data\\original\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "DATA_DIR = \"../data/original\"\n",
    "\n",
    "# create the data/original folder if it doesnt already exist\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# print a message to actually show the path so it can be verified\n",
    "print(f\"üìÇ Saving data to: {os.path.abspath(DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785384f3",
   "metadata": {},
   "source": [
    "### Fetch CelesTrak\n",
    "\n",
    "**CelesTrak** SATCAT.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d1321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_celestrak():\n",
    "    \"\"\"\n",
    "    Updates the local copy of satcat.csv\n",
    "    \"\"\"\n",
    "    print(\"--- Fetching CelesTrak (SATCAT) ---\")\n",
    "    url = \"https://celestrak.org/pub/satcat.csv\"\n",
    "\n",
    "    # join the file paths\n",
    "    save_path = os.path.join(DATA_DIR, \"satcat.csv\")\n",
    "\n",
    "    try:\n",
    "        # use requests to download the file, use stream=True for large files\n",
    "        response = requests.get(url, stream=True)\n",
    "        \n",
    "        # triggers an error if the link is broken\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # get the date the last time the file was updated\n",
    "        last_modified = response.headers.get(\"Last-Modified\")\n",
    "        if last_modified:\n",
    "            print(f\"üìÖ Server Last Update: {last_modified}\")\n",
    "\n",
    "        # no error has been thrown were good to save it.\n",
    "        with open(save_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "\n",
    "        # output save directory.\n",
    "        print(f\"‚úÖ Success! SATCAT saved to: {save_path}\")\n",
    "    except Exception as e:\n",
    "        # output the error message.\n",
    "        print(f\"‚ùå Error downloading CelesTrak: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d25c1",
   "metadata": {},
   "source": [
    "### Fetch CelesTrak\n",
    "\n",
    "**UCS** UCS-Satellite-Database 5-1-2023.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54f7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ucs():\n",
    "    print(\"\\n--- Fetching UCS Satellite Database ---\")\n",
    "    landing_page = \"https://www.ucsusa.org/resources/satellite-database\"\n",
    "\n",
    "    # we have to define these headers so the download can pretend to be a real browser/person\n",
    "    # then we use soup to read every line of the html\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Referer\": \"https://www.google.com/\" \n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"   üëÄ Scouting the landing page...\")\n",
    "        response = requests.get(landing_page, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"   ‚ùå Blocked! Status Code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        target_link = None\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            text = link.text.strip().lower()\n",
    "            if text == \"database\":\n",
    "                target_link = link['href']\n",
    "                print(f\"   üéØ Found the link: {target_link}\")\n",
    "                break\n",
    "        \n",
    "        if not target_link:\n",
    "            print(\"   ‚ùå Still could not find the link.\")\n",
    "            return\n",
    "\n",
    "        full_url = urljoin(landing_page, target_link)\n",
    "\n",
    "        print(\"   ‚¨áÔ∏è  Downloading the Excel file...\")\n",
    "        \n",
    "        file_response = requests.get(full_url, headers=headers, stream=True)\n",
    "        \n",
    "        last_modified = file_response.headers.get(\"Last-Modified\")\n",
    "        if last_modified:\n",
    "            print(f\"   üìÖ Server Last Update: {last_modified}\")\n",
    "            \n",
    "        filename_from_url = full_url.split(\"/\")[-1]\n",
    "        print(f\"   üè∑Ô∏è Remote Filename: {filename_from_url}\")\n",
    "        # ------------------------------------\n",
    "        \n",
    "        excel_path = os.path.join(DATA_DIR, \"UCS_raw.xlsx\")\n",
    "        \n",
    "        with open(excel_path, 'wb') as f:\n",
    "            for chunk in file_response.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "\n",
    "        print(\"   üîÑ Converting to CSV...\")\n",
    "        df = pd.read_excel(excel_path)        \n",
    "        csv_path = os.path.join(DATA_DIR, \"UCS-Satellite-Database.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        print(f\"   ‚úÖ Success! Saved to: {csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error with UCS data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2813f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch baseline countries GeoJSON for geopandas maps\n",
    "# thanksfully no scraping is required\n",
    "def fetch_countries_geojson():\n",
    "    print(\"\\n--- Fetching countries.geojson ---\")\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/johan/world.geo.json/master/countries.geo.json\"\n",
    "    save_path = os.path.join(DATA_DIR, \"countries.geojson\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=30)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "\n",
    "        last_modified = response.headers.get(\"Last-Modified\")\n",
    "\n",
    "        if last_modified:\n",
    "            print(f\"üìÖ Server Last Update: {last_modified}\")\n",
    "\n",
    "        print(f\"‚úÖ Success! GeoJSON saved to: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading GeoJSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a4ac3",
   "metadata": {},
   "source": [
    "### Execute Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af52c2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fetching UCS Satellite Database ---\n",
      "   üëÄ Scouting the landing page...\n",
      "   üéØ Found the link: /media/11492\n",
      "   ‚¨áÔ∏è  Downloading the Excel file...\n",
      "   üìÖ Server Last Update: Tue, 02 Jan 2024 14:39:30 GMT\n",
      "   üè∑Ô∏è Remote Filename: 11492\n",
      "   üîÑ Converting to CSV...\n",
      "   ‚úÖ Success! Saved to: ../data/original\\UCS-Satellite-Database.csv\n",
      "\n",
      "\n",
      "--- Fetching countries.geojson ---\n",
      "‚úÖ Success! GeoJSON saved to: ../data/original\\countries.geojson\n",
      "\n",
      "--- Fetching CelesTrak (SATCAT) ---\n",
      "üìÖ Server Last Update: Sun, 25 Jan 2026 20:32:22 GMT\n",
      "‚úÖ Success! SATCAT saved to: ../data/original\\satcat.csv\n"
     ]
    }
   ],
   "source": [
    "fetch_ucs()\n",
    "print()\n",
    "fetch_countries_geojson()\n",
    "print()\n",
    "fetch_celestrak()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
