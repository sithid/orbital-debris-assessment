{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f3c2c6",
   "metadata": {},
   "source": [
    "# SCRATCH PAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0728853",
   "metadata": {},
   "source": [
    "return project to more of a work in progress state, less polish more show your work.  now that we are in mod 3, there is inevitable changes to structure and pipeline (we have to build the database and schema stuff so the pipeline will have to change).  instructors need to be able to see my entire process, from raw code and markdown to polished version so they see and understand its not the AI thats doing the work, they need to see i understand what is going on and what i am doing.  git repo serves as the source of truth, i commit constantly so the evolution of the project should be well documented\n",
    "\n",
    "returned most of my 'thought process' comments within my code blocks.  markdown cells will remain, those took a long time to properly format all of the markdown, even with external markdown editors\n",
    "\n",
    "need to finish isolating the other 2 or 3 functions AI actually wrote for me and then its time to dive down the rabbit hole once again!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac584d7",
   "metadata": {},
   "source": [
    "it is drastically important to implement some form of automatic 'dirty' dataset updates (original source documents).  have to go back and update ALL of my findings, markdown, formulas, etc literally every time i find inconsistencies in the data is a serious issue that involves alot of work refactoring and updating everything.  need to implement auto update pipeline, and possibly auto-generated documentation updates.  for now ill settle for implementing an update notebook that updates the raw originals so i can keep the data pipeline up to date. allowing the datasets to become outdated by more than 6 months - 1 year can change results DRASTICALLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c11ffab",
   "metadata": {},
   "source": [
    "todo soon: remove original dataset files, force running pipeline_refresher to get original source documentation.  this enforces data freshness and my distribution concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99053c03",
   "metadata": {},
   "source": [
    "I realize i went DEEP down the rabbit hole on this, and way way past expectations, but i dont care.  this is my 4th code:you capstone project, not to mention what classes i passed with kctcs.  At this point, every capstone i complete should be techniqually better than the previous.  I need to be applying what I learn, from pathway to pathway, as I move forward.  I need to showcase my ability to incorporate transferable skills from one project to another, while adapting and evolving to every changing ecosystems, expectations, and standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a806d3e",
   "metadata": {},
   "source": [
    "being able to say \"30% of high-risk LEO satellites are registered in countries with minimal debris mitigation oversight\" adds a layer of depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e8d40",
   "metadata": {},
   "source": [
    "Current Focus: back up satcat_cleanup and start over.  need to focus on cleaning satcat dataset independing of ucs.  deep scrub satcat so that it is golden as a stand alone dataset, create a separate notebook to merge, augument, and polish a new 'master' dataset for our analysis'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e98cd",
   "metadata": {},
   "source": [
    "This is such a process. I make markdown cells, add code, and slog along.  Then I get to a point where my idea evolves, I have to go back through and change markdown and code to match the feel and vibe of my entire project.  This is actually a whole lot of fun.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85957220",
   "metadata": {},
   "source": [
    "The only purpose for this notebook is to house all of my random 'stuff' like code chunks, thoughts/comments, ideas etc.  No fancy formatting etc, just a paste pad that I can keep with ideas and other such things in raw form. Its like running commentary for my brain sometimes.\n",
    "\n",
    "I am pretty happy with where the project is at this point in time in comparison to the total amount of time I have before the end of the project.\n",
    "We are at the end of module 2, and we still have modules 3 and 4 to got.  I have so much time to continue exploring and visualing data, i'm really diggin this pathway.\n",
    "\n",
    "At the end of the day, the way my brain thinks about this project is that is just one big research project where I get to use real world data and various tools and utilities to deep dive on something I find interesting.  This has been alot of fun!\n",
    "\n",
    "Thanks to having to learn markdown for the bazillion readmes ive had to make, up to this point, for the 3 - 4 pathway/workshops ive taken, markdown has become infinitely more useful with jupiter notebooks.  Markdown cells truely are superior to code comments both in quality and the level of readibility they provide. It's little readme chunkies whenever i want them. It's almost stupid how full circle thats come, from being forced to make readmes for 4 capstone projects now and hating every second of it, to actually being greatful for the ability to use it through-out my notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbea972",
   "metadata": {},
   "source": [
    "make sure we do this in a way where we clean everything, not just what we need right now from this version of the datasets.  set up proper cleaning pipelines incase future changes to the datasets introduce bad values for various columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c539aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports i may need for various code blocks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from shapely.geometry import Point, Polygon\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b927ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file('../data/original/countries.geojson')\n",
    "kinetic = pd.read_csv('../data/clean/kinetic_master.csv', low_memory=False)\n",
    "satcat = pd.read_csv('../data/original/satcat.csv', low_memory=False)\n",
    "ucs = pd.read_csv('../data/clean/ucs_cleaned.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e303e",
   "metadata": {},
   "source": [
    "### **Legacy Cadence: The Steady March**\n",
    "\n",
    "represents the rhythmic pattern of the space industry as it existed from the dawn of the space age through approximately 2013\n",
    "\n",
    "constant drumbeat\n",
    "\n",
    "### **Modern Cadence: The Sprinting Pulse**\n",
    "\n",
    "represents the fundamental shift in the industry's tempo that occurred with the rise of commercial mega-constellations\n",
    "\n",
    "heart rate in a sprint\n",
    "\n",
    "it is an accelerating pulse where the time between launches is rapidly shrinking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af549510",
   "metadata": {},
   "source": [
    "### Math\n",
    "**First Derivative**\n",
    "\n",
    "In calculus, the \"first derivative\" measures the speed of change.  \n",
    "\n",
    "\n",
    "Comparing how fast launches were increasing in the old model (legacy linear) versus how fast they are increasing in the new model (modern exponential).\n",
    "\n",
    "We use Brent's Method to find the exact year where the modern exponential growth rate officially surpassed the legacy linear growth rate.\n",
    "\n",
    "Calculating the slopes lets us compare how fast launches were increasing in the old model versus how fast they were increasing in the new one.\n",
    "\n",
    "### **[Brent's Method](https://en.wikipedia.org/wiki/Brent%27s_method)**\n",
    "\n",
    "\n",
    "Using Brent's Method to find the exact moment where the two speeds (slopes) became equal (2014) and we interpret this as the year the transition occurs (root), because the models velocity officially surpasses the legacy trend during that calendar year.\n",
    "\n",
    "- growth models\n",
    "  * two distinct mathematical functions help us characterize the two different eras of space flight:\n",
    "    * steady state (linear growth): $y = mx + b$  \n",
    "      * $m$ is the constant slope (legacy launch cadence) and $b$ is the intercept.\n",
    "    * new space (exponential growth): $y = a \\cdot e^{b(x-c)}$  \n",
    "      * $a$ is the scale, $b$ is the growth rate, and $x-c$ is the time elapsed since the baseline year ($c$).\n",
    "- acceleration math\n",
    "  * exponential slope (($f'(x)$)): $\\frac{d}{dx}[a \\cdot e^{b(x-c)}] = a \\cdot b \\cdot e^{b(x-c)}$\n",
    "    * calculates the exact slope of the exponential curve at any given year  \n",
    "- root finding solver\n",
    "  * root: exact year where the  exponential growth rate ($f'(x)$) officially surpassed the legacy cadence\n",
    "\n",
    "[Derivatives of Exponential Functions - YouTube](https://www.youtube.com/watch?v=S0_qX4VJhMQ)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41baa6",
   "metadata": {},
   "source": [
    "### Double Threat (The Pincer Maneuver)\n",
    "The \"Double Threat\" is the fact that these two populations are physically segregating the usable space in LEO:\n",
    "\n",
    "- **The Floor is Rising**:\n",
    "  - The \"**Commuter Lane**\" is becoming so dense with active satellites that the probability of \"active-on-active\" collisions is skyrocketing.\n",
    "- **The Ceiling is Heavy**:\n",
    "  - The \"**Deadly Ring**\" looms directly above the active constellations. If one of those massive legacy objects breaks apart, the resulting debris cloud will \"rain down\" into the Commuter Lane.\n",
    "\n",
    "### Double Threat: What It Means\n",
    "\n",
    "**The Double Threat** means the \"Runner\" (**Modern Cadence**) is sprinting through a \"**Canyon**\" where the walls (**Legacy Cadence**) are made of unmonitored, multi-ton kinetic bombs. You aren't just looking at \"**more junk**\" — you are looking at two different eras of risk colliding in the same vertical space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15913cd",
   "metadata": {},
   "source": [
    "### Supporting Information - Fact Checking, Math Research, Number Verifications\n",
    "\n",
    "various fact checking, math research, algorithm info, number verifications etc  \n",
    "i dont like magic numbers, i want to understand where they come from, why they are important, why we use them, etc etc\n",
    "\n",
    "### **2014 is the year the \"Modern Sprint\" (exponential growth) officially outpaced the \"Legacy March\" (linear growth)**\n",
    "* External data and industry analysis confirm that 2014 was indeed the commencement of the \"accelerating growth pattern\" that defines the current era.  \n",
    "* According to the Union of Concerned Scientists (UCS), the dramatic, skyrocketing pattern of active satellites is a trend that \"has been building since 2014.\"  \n",
    "* 2014 was a pivotal year for the \"New Space\" economy. This was the year that commercial satellite operators began shifting from large, singular Geosynchronous (GEO)   satellites to the early deployments of Low Earth Orbit (LEO) constellations.\n",
    "  * In early 2014, major shifts in commercial earth observation and infrared imaging (like the Hera Systems 1HOPSAT-TD) began appearing in the UCS database, signaling the move toward smaller, high-frequency deployments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0107ae",
   "metadata": {},
   "source": [
    "### Understanding The Math\n",
    "### **Pillar 1: The Characterization (Curve Fitting)**\n",
    "\n",
    "Before you can find the \"Acceleration Point,\" you have to define the two different \"Beats\" (Cadences) of the industry. We use **Non-Linear Least Squares** to do this.\n",
    "\n",
    "*   **The Linear Model (The Steady March):** $y = mx + b$ (Slope-Intercept Form)\n",
    "    \n",
    "    *   **The Math:** This assumes every year adds the same number of satellites ($m$). We fit this to the data from 1957 to 2013.\n",
    "        \n",
    "*   **The Exponential Model (The Sprinting Pulse):** $y = a \\cdot e^{b(x-c)}$ (Exponential Growth/Decay Model)\n",
    "    \n",
    "    *   **The Math**:\n",
    "        * This assumes the growth rate is proportional to the current population. The more satellites we have, the faster we can launch more.\n",
    "            * \\($a$\\) represents the initial amount (when \\(x=c\\)), \\(b\\) is the rate constant, and \\(c\\) is a horizontal shift (or phase shift).\n",
    "    *   **Varibles**:\n",
    "        * \\($y$\\) The Orbital Population, total number of active satellites in orbit for a given year\n",
    "        * \\($a$\\) The Initial Scale (Starting Volume), volume of the satellite population at the very beginning of the \"New Space\" search window. It sets the \"starting line\" for the exponential curve before     the growth rate takes over.\n",
    "        * \\($e$\\) Euler's Number (~2.718), The mathematical constant for natural growth, represents the idea of continuous growth.\n",
    "        * \\($b$\\) The Growth Coefficient (The \"Sprint\" Intensity)\n",
    "            * If $b$ is high, the \"heart rate\" of the sprint is faster, and the curve turns upward sharply.\n",
    "            * The $b$ value is what makes the \"Modern Sprint\" faster than the \"Legacy March.\" It represents the aggressive deployment cycles of commercial mega-constellations.\n",
    "        * \\($x$\\) The Time (The Independent Variable), The input value on the x axis, the calendar year.\n",
    "        * The Time Offset (The Inflexion Baseline)\n",
    "            * horizontal shift in the curve often used to \"center\" the math on a specific starting year (like 2000). \n",
    "            * It ensures the math doesn't try to calculate an exponential curve starting from Year 0, which would crash the model. It anchors analysis to the modern era.\n",
    "        * Notes: \\($b$\\) is the intensity of the commercial sprint, $a$ is where we started, and $c$ is the anchor that lets us compare the modern era to the legacy past.\n",
    "\n",
    "*   **The Tool (curve\\_fit):** This is an optimization algorithm. It tries thousands of different values for $a$, $b$, and $m$ until it finds the version of the line that has the **least amount of error** compared to your actual UCS data.\n",
    "\n",
    "### **Pillar 2: The Comparison (Derivatives)**\n",
    "\n",
    "To find the \"Double Threat,\" we stop looking at the **total number** of satellites (Volume) and start looking at the **speed of change** (Velocity). In math, the \"Speed of a Curve\" is the **Derivative**.\n",
    "\n",
    "*   **Linear Velocity:** The derivative of $mx + b$ is just $m$. It's a constant speed. The \"drumbeat\" never changes.\n",
    "    \n",
    "*   **Exponential Velocity:** The derivative of $a \\cdot e^{b(x-c)}$ is $a \\cdot b \\cdot e^{b(x-c)}$.\n",
    "    \n",
    "    *   **The Critical Realization:** Because the variable $x$ (the year) is still in the exponent, the _speed itself_ is getting faster every single day.\n",
    "\n",
    "### **Pillar 3: The Discovery (Brent's Method)**\n",
    "\n",
    "This is where you find the **2014 Pivot**. You want to know exactly when the \"Sprint\" started moving faster than the \"March.\"\n",
    "\n",
    "*   The Equation: We set the two velocities equal to each other: $$f'(x) = m$$ $$\\text{Speed of Sprint} = \\text{Speed of March}$$\n",
    "    \n",
    "*   **The Problem:** You can't always solve this with basic algebra because $x$ is stuck inside an exponent.\n",
    "    \n",
    "*   **The Solution (brentq):** This is a **Root-Finding Algorithm**. It looks for the \"Root\" (the zero-point) of the equation: $f'(x) - m = 0$.\n",
    "    \n",
    "    *   Brent's Method is \"Smart.\" It uses a **Bisection** (chopping the search area in half) to stay safe, but uses **Interpolation** (calculating the trajectory) to move fast.\n",
    "        \n",
    "    *   **The Result:** It spits out **2014.58**. This is your mathematical proof that mid-2014 was the moment the \"Runner\" officially took off.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc08ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "# AI Asissted Reporting Template\n",
    "#\n",
    "# When I first realized how badly I needed some kind of decent diagnostic reporting,\n",
    "# I asked AI to suggest a report format/structure that would be informative and simple in nature.\n",
    "# This is more or less the structure that I use. It changes some depending on exactly what kind of diagnostic\n",
    "# report im tryingg to make. Its not all that complex I'm just not very good at making things look 'pretty' \n",
    "# and I really wanted a report format I could standardize.\n",
    "#\n",
    "# This template style is fairly strait forward, we use f-strings to build the report text,\n",
    "# and we loop through a list of columns to dynamically build out the data quality section.\n",
    "# Each section is clearly marked so its easy to find and modify later.\n",
    "#\n",
    "# I will be coming back to this, to refactor the template idea into a reusable function or maybe\n",
    "# I will build a reporting class. For now, this template works as a starting point for each new report.\n",
    "###########################################################################################################\n",
    "'''\n",
    "# ==========================================\n",
    "# 1. PREP & FILTERS (The \"Setup\")\n",
    "# ==========================================\n",
    "# Define your core dataframe and primary filter (e.g., 'Active', 'Sold', 'In-Orbit')\n",
    "target_df = df  # or df[df['status'] == 'Active']\n",
    "total_records = len(df)\n",
    "target_count = len(df)\n",
    "\n",
    "# ==========================================\n",
    "# 2. CALCULATE SCALARS (The \"KPIs\")\n",
    "# ==========================================\n",
    "# metrics: simple sums, means, or divisions\n",
    "primary_metric_sum = target_df['primary_column'].sum()\n",
    "secondary_metric_avg = target_df['secondary_column'].mean()\n",
    "\n",
    "# breakdown: counts for specific categories etc\n",
    "cat_a_count = (target_df['category_col'] == 'Category A').sum()\n",
    "cat_b_count = (target_df['category_col'] == 'Category B').sum()\n",
    "\n",
    "# timeline: min/max dates - temporal data\n",
    "start_date = target_df['date_col'].min()\n",
    "end_date = target_df['date_col'].max()\n",
    "\n",
    "# ==========================================\n",
    "# 3. DEFINE AUDIT SCOPE (The \"Config\")\n",
    "# ==========================================\n",
    "# List of columns you want to check for quality/completeness\n",
    "audit_columns = [\n",
    "    'id_col', 'category_col', 'value_col', \n",
    "    'date_col', 'status_col', 'notes_col'\n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# 4. GENERATE REPORT STRING (The \"Document\")\n",
    "# ==========================================\n",
    "# This f-string builds the header and static tables\n",
    "report = f\"\"\"\n",
    "### **Pipeline Diagnostic Report**\n",
    "**Dataset Scope:** {total_records:,} Total Records ({target_count:,} in Target Scope)\n",
    "\n",
    "#### **Timeline Summary**\n",
    "| Metric | Value |\n",
    "| :--- | :--- |\n",
    "| **Earliest Record** | {start_date} |\n",
    "| **Latest Record** | {end_date} |\n",
    "| **Duration** | {end_date - start_date} |\n",
    "\n",
    "#### **Key Performance Indicators**\n",
    "| Metric | Value | Context |\n",
    "| :--- | :--- | :--- |\n",
    "| **Primary Total** | {primary_metric_sum:,.0f} | Raw sum of primary column |\n",
    "| **Secondary Avg** | {secondary_metric_avg:.2f} | Average of target scope |\n",
    "| **Target Share** | {(target_count / total_records):.1%} | % of total data retained |\n",
    "\n",
    "#### **Categorical Breakdown**\n",
    "| Category | Count | Share |\n",
    "| :--- | :--- | :--- |\n",
    "| **Category A** | {cat_a_count:,} | {cat_a_count/target_count:.1%} |\n",
    "| **Category B** | {cat_b_count:,} | {cat_b_count/target_count:.1%} |\n",
    "| **Other** | {target_count - (cat_a_count + cat_b_count):,} | - |\n",
    "\n",
    "#### **Data Density & Quality Audit**\n",
    "| Feature | Completeness | Data Type | Status |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# 5. DYNAMIC ROWS (The \"Loop\")\n",
    "# ==========================================\n",
    "# Iterate through your config list to append rows to the table\n",
    "for col in audit_columns:\n",
    "    # 1. Calculate coverage (not null %)\n",
    "    completeness = target_df[col].notna().mean()\n",
    "    \n",
    "    # 2. Determine \"Method\" or \"Type\" (Custom logic based on column name)\n",
    "    if 'id' in col:\n",
    "        dtype = \"ID / Key\"\n",
    "    elif 'date' in col:\n",
    "        dtype = \"Temporal\"\n",
    "    else:\n",
    "        dtype = \"Attribute\"\n",
    "    \n",
    "        \n",
    "    # 3. Append the row to the report string\n",
    "    report += f\"| **{col.replace('_', ' ').title()}** | **{completeness:.1%}** | {dtype} | {'✅' if completeness > 0.9 else '⚠️'} |\\n\"\n",
    "\n",
    "# ==========================================\n",
    "# 6. RENDER & EXPORT (The \"Delivery\")\n",
    "# ==========================================\n",
    "display(Markdown(report))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001024ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how i can use the report.\n",
    "df = pd.read_csv('../data/clean/kinetic_master.csv', low_memory=False)\n",
    "rcs_df = df[df['rcs_class'] != 'UNKNOWN']\n",
    "global_record_count = len(df)\n",
    "subset_record_count = len(rcs_df)\n",
    "\n",
    "\n",
    "rcs_group = rcs_df['rcs_class'].value_counts().reset_index()\n",
    "rcs_group.columns = ['rcs_class', 'count']\n",
    "display(rcs_group)\n",
    "rcs_df = rcs_df.groupby('rcs_class')['owner'].count().reset_index()\n",
    "\n",
    "display(rcs_group)\n",
    "\n",
    "\n",
    "# only worry about objects that have already deorbited\n",
    "subset_df_sum = rcs_df['launch_mass_kg'].count()\n",
    "subset_df_avg = rcs_df['launch_mass_kg'].dropna().mean()  \n",
    "\n",
    "cols = ['launch_mass_kg']\n",
    "\n",
    "report = f\"\"\"\n",
    "### **Pipeline Diagnostic Report**\n",
    "**Primary Dataset:** {global_record_count:,} Total Records   \n",
    "  **Target Subset:** {subset_record_count:,} Subset Records   \n",
    "  **Target Share:** {(subset_record_count / global_record_count):.1%}   \n",
    "\n",
    "#### **Misc Summary**\n",
    "| Metric | Value |\n",
    "| :--- | :--- |\n",
    "| **Small Count** | {subset_record_count:,} |\n",
    "| **Small Mass Sum** | {subset_df_sum:,.1f} |\n",
    "| **Small Mass Avg** | {subset_df_avg:.2f} |\n",
    "\n",
    "#### **Subset Data Density & Quality Audit**\n",
    "| Feature | Completeness | Data Type | Status |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "\"\"\"\n",
    "\n",
    "for col in cols:\n",
    "  # notna returns a series of booleans where True = not null for the specific column for every row\n",
    "  # notna().mean counts up the number of true values and divides by the total number of rows which will \n",
    "  # give the percentage of non-null (completeness/filled) values in that column\n",
    "  completeness = rcs_df[col].notna().mean()\n",
    "  report += f\"| **{col.replace('_', ' ').title()}** | **{completeness:.1%}** | {rcs_df[col].dtypes} | {'✅' if completeness > 0.9 else '⚠️'} |\\n\"\n",
    "\n",
    "display(Markdown(report))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
