{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f3c2c6",
   "metadata": {},
   "source": [
    "# PASTE PAD\n",
    "\n",
    "The only purpose for this notebook is to house all of my random 'stuff' like code chunks, thoughts/comments, ideas etc.  No fancy formatting etc, just a paste pad that I can keep with ideas and other such things in raw form. Its like running commentary for my brain sometimes.\n",
    "\n",
    "I am pretty happy with where the project is at this point in time in comparison to the total amount of time I have before the end of the project.\n",
    "We are at the end of module 2, and we still have modules 3 and 4 to got.  I have so much time to continue exploring and visualing data, i'm really diggin this pathway.\n",
    "\n",
    "At the end of the day, the way my brain thinks about this project is that is just one big research project where I get to use real world data and various tools and utilities to deep dive on something I find interesting.  This has been alot of fun!\n",
    "\n",
    "Thanks to having to learn markdown for the bazillion readmes ive had to make, up to this point, for the 3 - 4 pathway/workshops ive taken, markdown has become infinitely more useful with jupiter notebooks.  Markdown cells truely are superior to code comments both in quality and the level of readibility they provide. It's little readme chunkies whenever i want them. It's almost stupid how full circle thats come, from being forced to make readmes for 4 capstone projects now and hating every second of it, to actually being greatful for the ability to use it through-out my notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c539aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports i may need for various code blocks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19c90f",
   "metadata": {},
   "source": [
    "Basic geopanda example to make sure we got everything installed in the virtual environment etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e326a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the countries.geojson\n",
    "try:\n",
    "    world = gpd.read_file('../data/original/countries.geojson')\n",
    "    print(\"SUCCESS: Modern GeoJSON map loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Check your 'data/original/' folder. Is the file named 'countries.geojson'?\")\n",
    "\n",
    "# 2. Setup your project test data\n",
    "data = {\n",
    "    'Satellite_ID': ['Test_Point_KY'],\n",
    "    'Lat': [37.99],\n",
    "    'Lon': [-84.17]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 3. Create the GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df, \n",
    "    geometry=[Point(xy) for xy in zip(df['Lon'], df['Lat'])], \n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# 4. Final Visualization\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Plot the new GitHub countries data\n",
    "world.plot(ax=ax, color='#f2f2f2', edgecolor='#adadad')\n",
    "\n",
    "# Plot your test point\n",
    "gdf.plot(ax=ax, color='red', markersize=200, label='Satellite Tracking Point')\n",
    "\n",
    "plt.title(\"LEO Ground Track Environment: Verified (GitHub Geo-Countries Source)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce0590",
   "metadata": {},
   "source": [
    "# REORGANIZE\n",
    "\n",
    "* I seem to have a few cleanup steps and maybe some data augmentation in my eda book(s) that may be better left in the cleanup notebook. \n",
    "* Considering strickly cleaning satcat.csv and saving a cleaned version of that, and then creating a 3rd notebook speecifically for the csv created by merging my 2 datasets.  This should allow me to maintain the integrity of the original data after cleaning that is ready to be used to create a polished dataset. The 3rd notebook would contain the merging and subsuquent cleaning/augmentation steps for the final polished dataset for the merged data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
